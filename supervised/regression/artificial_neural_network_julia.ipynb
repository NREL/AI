{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bac2a43-c248-498b-bc5f-cce8a2f3a066",
   "metadata": {},
   "source": [
    "This tutorial will walk you through how to perform a regression task using supervised machine learning (ML) methods on different types of data that might be relevent for various scientific applications. First we must generate some representative data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27de832-daa9-49b5-9e52-6ce2cec763eb",
   "metadata": {},
   "source": [
    "## Generating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5192f418-2441-4255-8fd9-f44a56f8408a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate working environment\n",
    "using Pkg; Pkg.activate(\"../\")\n",
    "# using Pkg; Pkg.add(\"Plots\")\n",
    "# using Pkg; Pkg.add(\"MLUtils\")\n",
    "# using Pkg; Pkg.add(\"Flux\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47b2b1b-2757-41df-bd59-4b564bde7ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random \n",
    "using Plots\n",
    "using Statistics\n",
    "using Flux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5418d064-8f53-4ced-93b1-536282b14b16",
   "metadata": {},
   "source": [
    "First, we will generate some nonlinear multi-variate data that will serve as a sufficiently challenging test case for our ML model, we will use the Friedman #1 regression function which takes the form: $$f(x)=10sin(\\pi x_1 x_2)+ 20(x_3−0.5)^2+10x_4 + 5x_5+ N(0,\\sigma) $$\n",
    "\n",
    "In the following distribution we will create output with 10 features despite the Friedman #1 function only depending upon the first 5 features. We will also add noise to the data to see how well the neural network models can fit perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be43298-9ec4-481e-9bb4-ceb36b51ea6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "function make_friedman1(n_samples::Int64, n_features::Int64, noise::Float64, random_state::Int64)\n",
    "    rng = Random.seed!(random_state)\n",
    "    X = rand(rng, Float32, (n_features, n_samples))\n",
    "    y = Array{Float32}(undef, (n_samples))\n",
    "    @. y = 10sin(π*X[1, :]*X[2, :]) + 20(X[3, :]-0.5)^2 + 10X[4, :] + 5X[5, :] \n",
    "    y.+= noise.*randn(rng, Float32, (n_samples))\n",
    "    return (X, reshape(y, 1, n_samples))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7e3b58-376b-4cb7-8ffd-f20074b4bd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a non-linear distribution using the Friedman #1 function\n",
    "n_samples = 1000\n",
    "n_feats = 10\n",
    "x, y = make_friedman1(n_samples, n_feats, 0.0, 42)\n",
    "println(\"Data size: \", size(x), size(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7b5d07-275c-4f58-938f-dcd56f9568bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data using a mosaic plot\n",
    "titles = permutedims(collect('A':'I'))\n",
    "scatter(transpose(x[1:9, :]), transpose(y), markersize=2, layout=9, legend=false, title=titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f752a2a5-5a62-4243-a5b4-e9f3af767d04",
   "metadata": {},
   "source": [
    "## Preprocessing the data\n",
    "Most ML problems optimally operate on data that is normalized. The optimal type of normalization depends upon the structure of the data and the desired output.\n",
    "\n",
    "For this data, we will use a rather standard that shifts and scales the data to a distribution centered around 0 with a standard deviation equal to 1.\n",
    "\n",
    "$$ x' = \\frac{x - \\mu(x)}{\\sigma(x)}$$\n",
    "\n",
    "We can do this manually, or use a number of pre-defined functions in sklearn. Here we will manually define the normalization functions, but in other tutorials we can explore different methods.\n",
    "\n",
    "When creating a normalization function, it is often useful to also create an inverse normalization function which can be used to convert the ML model predictions back to the physical space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d91185-f5ba-4fa2-bd4b-2792c1a6ffa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "function normalize(X)\n",
    "    μ = mean(X, dims=2)\n",
    "    σ = stdm(X, μ, dims=2)\n",
    "    X_normalized = (X.-μ)./σ\n",
    "    return X_normalized, μ, σ\n",
    "end\n",
    "\n",
    "inverse_normalize(X_normalized, μ, σ) = X_normalized.*σ.+μ;\n",
    "\n",
    "x_norm, x_mean, x_std = normalize(x);\n",
    "y_norm, y_mean, y_std = normalize(y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53285bd2-8abc-41de-b5cb-15702d8ea463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the normalized data \n",
    "scatter(transpose(x_norm[1:9, :]), transpose(y_norm), markersize=2, layout=9, legend=false, title=titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8379423d",
   "metadata": {},
   "source": [
    "## Creating Train, Validation, and Test Datasets\n",
    "\n",
    "We will now divide our dataset into three subdatasets for training, validation, and testing.\n",
    "- Training Dataset: The dataset that we will use to optimize our ML model.\n",
    "- Validation Dataset: A small, unseen dataset that we will use to monitor the performance of the ML model during training time. This can be useful for tuning hyperparameters and determining if the ML model is overfitting to the seen data.\n",
    "- Test Dataset: Unseen data that is used to determine the overall performance of the fully-trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbaa478-5ec7-4b85-beec-e67bf12b4f4e",
   "metadata": {},
   "source": [
    "Now we can generate training, validation and testing datasets with 80%-10%-10% split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ebe695-4eb1-4024-a283-c40967797268",
   "metadata": {},
   "outputs": [],
   "source": [
    "using MLUtils\n",
    "train_data, val_data, test_data = splitobs((x_norm, y_norm); at=(0.8, 0.1) , shuffle=true);\n",
    "println(\"Train data size: \", size(train_data[1]), size(train_data[2]))\n",
    "println(\"Validation data size: \", size(val_data[1]), size(val_data[2]))\n",
    "println(\"Test data size: \", size(test_data[1]), size(test_data[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1e104a-998d-4cf2-b1b4-076f632eedaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# precompute the unnormalized versions of y\n",
    "y_train_real = inverse_normalize(train_data[2], y_mean, y_std);\n",
    "y_val_real = inverse_normalize(val_data[2], y_mean, y_std);\n",
    "y_test_real = inverse_normalize(test_data[2], y_mean, y_std);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc8ec0f-d2db-4a3c-8d92-ed222fbe8cad",
   "metadata": {},
   "source": [
    "## Creating and training an ML Model\n",
    "Now that the data is preprocessed nicely, let's create a standard feed-forward neural network and train it to learn the relationship between the features ($x$) and the output ($y$).\n",
    "\n",
    "The number of hidden layers, neurons per layer, and a variety of other so-called \"hyperparameters\" will have a noticeable impact on model results. These values require extensive tuning depending on the problem at hand. For this example: \n",
    "\n",
    "- The initial model has an input layer, three hidden layers with number of neurons defined by the `width`, and an output layer, all layers with ReLU activation function.\n",
    "\n",
    "- We also need to define the loss function, e.g. we can use the \"mean square error\" function (MSE) (which in `Flux` is named **`Flux.mse`**):\n",
    "$$L(x, y) = \\frac{1}{n}\\sum_i^n \\left[y_i - model(x_i) \\right]^2, $$ \n",
    "\n",
    "- `Flux` also brings in lots of optimizers. For this example, we pick the `Adam` optimizer.\n",
    "\n",
    "- Finally, to train the model, we will use batches of 32 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd568d0b-cb75-4385-bf87-5d7f1b709ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 8\n",
    "model = Chain(\n",
    "    Dense(n_feats => width, relu),   # activation function inside the layer\n",
    "    Dense(width => width, relu),\n",
    "    Dense(width => width, relu),\n",
    "    Dense(width => 1))               # output layer with one feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b762de36-ad47-46cb-bfce-3e68cd159273",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 32\n",
    "# Define the loss function \n",
    "L(m, x, y) = Flux.mse(m(x), y)\n",
    "# Define optimizer\n",
    "optim = Flux.setup(Flux.Adam(learning_rate, (0.9, 0.8)), model); \n",
    "# Define the batch loader\n",
    "loader = Flux.DataLoader(train_data, batchsize=batch_size, shuffle=false);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498d37c8",
   "metadata": {},
   "source": [
    "## Training the ML Model\n",
    "Finally, we will train the ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b89f7a-dfbb-4d60-b2a5-e31cb317c6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "# Make a history arrays for storing metrics\n",
    "train_loss = Float64[]\n",
    "val_loss = Float64[]\n",
    "\n",
    "# Training for 100 epoch and saving training and testing loss\n",
    "@time for epoch in 1:num_epochs\n",
    "    for batch in loader\n",
    "        Flux.train!(L, model, [batch], optim)\n",
    "    end\n",
    "    push!(train_loss, L(model, train_data[1], train_data[2]))\n",
    "    push!(val_loss, L(model, val_data[1], val_data[2]))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fc8f9f-b91d-4eee-9f05-6e7768666693",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(train_loss; lw=3, xaxis=(\"epochs\"), yaxis=\"loss\", label=\"train loss\")\n",
    "plot!(val_loss; lw=3, label=\"validation loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924317f1-7aa9-41ce-a552-6042cbc9dfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"Final train loss: $(train_loss[end])\")\n",
    "println(\"Final validation loss: $(val_loss[end])\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab395ccf-dfae-4fe8-b1ac-243033757967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on train and test data\n",
    "pred_norm = model(x_norm);\n",
    "pred_norm_train = model(train_data[1]);\n",
    "pred_norm_val = model(val_data[1]);\n",
    "pred_norm_test = model(test_data[1]);\n",
    "\n",
    "# Unscale the results\n",
    "pred_full = inverse_normalize(pred_norm, y_mean, y_std);\n",
    "pred_train = inverse_normalize(pred_norm_train, y_mean, y_std);\n",
    "pred_val = inverse_normalize(pred_norm_val, y_mean, y_std);\n",
    "pred_test = inverse_normalize(pred_norm_test, y_mean, y_std);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b15f2c-cf14-4653-8a93-fac7e0e21d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(transpose(x[1:9, :]), transpose(y), markersize=2, layout=9, legend=false, title=titles)\n",
    "scatter!(transpose(x[1:9, :]), transpose(pred_full), markersize=2, layout=9, legend=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ff67c6-afa8-497e-846f-e0e77739eecd",
   "metadata": {},
   "source": [
    "## Compute some statistics on the fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90d9868-8b2f-4e7a-87e3-53f60e3f9249",
   "metadata": {},
   "outputs": [],
   "source": [
    "function prediction_stats(pred, truth)\n",
    "    rss = sum((pred .- truth).^2)\n",
    "    tss = sum((truth .- mean(truth, dims=2)).^2)\n",
    "    r_sq = 1 - rss/tss\n",
    "    rmse = sqrt(Flux.mse(pred, truth))\n",
    "    return r_sq, rmse\n",
    "end\n",
    "\n",
    "r_sq_train, rmse_train = prediction_stats(pred_train, y_train_real);\n",
    "r_sq_val, rmse_val = prediction_stats(pred_val, y_val_real);\n",
    "r_sq_test, rmse_test = prediction_stats(pred_test, y_test_real);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d114bd2d-c357-4ce7-8571-e9be5a0ead32",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = scatter(transpose(y_train_real), transpose(pred_train), markershape=:star5, label=\"prediction\", ylabel=\"prediction\", xlabel=\"truth\", title=\"Training Data\")\n",
    "plot!(p1, transpose(y_train_real), transpose(y_train_real), linestyle=:dash, label=\"x=y\", color=\"grey\")\n",
    "annotate!(p1, 2.5, 21, Plots.text(\"R^2 = $(round(r_sq_train, digits=2)),\\nRMSE = $(round(rmse_train, digits=2))\", :left, 12))\n",
    "\n",
    "p2 = scatter(transpose(y_val_real), transpose(pred_val), markershape=:star5, label=\"prediction\", xlabel=\"truth\", title=\"Validation Data\")\n",
    "plot!(p2, transpose(y_val_real), transpose(y_val_real), linestyle=:dash, label=\"x=y\", color=\"grey\")\n",
    "annotate!(p2, 2.5, 21, Plots.text(\"R^2 = $(round(r_sq_val, digits=2)),\\nRMSE = $(round(rmse_val, digits=2))\", :left, 12))\n",
    "\n",
    "p3 = scatter(transpose(y_test_real), transpose(pred_test), markershape=:star5, label=\"prediction\", xlabel=\"truth\", title=\"Test Data\")\n",
    "plot!(p3, transpose(y_test_real), transpose(y_test_real), linestyle=:dash, label=\"x=y\", color=\"grey\")\n",
    "annotate!(p3, 2.5, 21, Plots.text(\"R^2 = $(round(r_sq_test, digits=2)),\\nRMSE = $(round(rmse_test, digits=2))\", :left, 12))\n",
    "\n",
    "p = plot(p1, p2, p3, layout = (1, 3), link=:all,  size=(1200, 350))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bf29d1",
   "metadata": {},
   "source": [
    "## Exploration\n",
    "Exploration of different hyperparameters can have a large impact on the training of the model. These are some examples of hyperparameters that can be examined in more detail:\n",
    "\n",
    "- Hyperparameters to explore\n",
    "- Model depth and width\n",
    "- Layer regularization\n",
    "- Layer initialization\n",
    "- Learning rate\n",
    "- Batch normalization\n",
    "- Dropout layers\n",
    "- Activation functions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.1",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
